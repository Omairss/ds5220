---
title: "Identifying Drawings with the Quick Draw Challenge"
author: "Group 3"
output: 
  pdf_document:
    keep_tex: yes
bibliography: references.bib
---

We're trying to put together a CNN based on the "Quick, Draw!" Kaggle
competition [@QuickDra10].

# Introduction

Provide a short background of the project (e.g., what kind of question is to be 
answered with this work). Provide a short non-technical summary of your 
analytical strategy.


Our team has based our project idea on the "Quick, Draw!" Kaggle competition 
which classifies user drawings into one of 345 label categories. Many drawings 
were incomplete or failed to match the label. The challenge was to effectively 
build a recognizer that could work with this noisy data. The challenge was unique 
from an algorithmic perspective because the data included both temporal and 
spatial components; areas that have traditionally used two different 
types of Neural Network Architectures. Our team attempted to provide a solution 
using a subset of the data, to the Kaggle problem, while taking a more in depth 
look at potential pitfalls and modeling strategy, without having to use 
extensive computational resources. With this project, we explore image 
classification in depth. We try out various experiments like fully connected 
neural network vs CNN, bias vs. variance tradeoff, overfitting data purposefully
and trying out techniques like regularization and dropout to fix overfitting and
finally, testing neural network with imbalanced data.


# Methods

First, let's talk about data collection. Google initially compressed and converted
the images to .npy files. These .npy files are basically 28*28 images, containing
more than 100,000 rows each.

So here are the experiments we performed.

### EDA

We checked the We plotted images from several categories 

$H1_a:$ Prediction will go up when we use a CNN compared to a fully
	connected feedforward neural network.

$H2_a:$ Prediction will go up when we use regularization.

$H3_a:$ Run neural networks without any activations.

$H4_a:$ Try different architectures- number of layers, different (or no) 
activations

$H5_a:$ CNN with dilation is preferable to LSTM because it's presumably
faster. We can cite some papers here.

Try out: Overfitting, Bias/variance, imbalance, dropout, pushing activation

# Results

Here's how the results performed when we were exploring our 
hypotheses

| Hypothesis | Result         |
| ---------- | -------------- |
| $H1_a$     | Accept         |
| $H2_a$     | Fail to Accept |
| $H3_a$     | Fail to Accept |
| $H4_a$     | Accept         |
| $H5_a$     | Fail to Accept |

## Fully Connected Neural Network

Here's the run down on it and how it relates
to the hypotheses. 

## Convolutional Neural Network

same thing for the CNN. TOOD: can we fold in some of
the specializations into this section??

# Discussion

Here's what worked the best, worst, and future areas for research.

The model built using CNN was better than human accuracy, which was in turn 
better than the model built using fully connected network.

# References

# Appendix

Add computer code, plots, and other relevant technical details that will help
me evaluating your work.

# Statement of contribution

Everyone worked on everything.


