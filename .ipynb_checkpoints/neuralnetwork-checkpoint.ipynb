{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network\n",
    "\n",
    "Please note that I've already completed the \"Neural Networks and Deep Learning\" course from Andrew Ng. This is the course which was recommended in class notes. Much of my code will look similar to what I did for that assignment. However, the activation functions and their use in backpropagation will be unique. I'm kind of overbuilding in some ways but this serves the purpose of being able to use my implementation here to complete a stage of our research project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network Implementation\n",
    "\n",
    "The following functions implement a Feedforward Neural Network. These\n",
    "functions are broken out into a few sections\n",
    "\n",
    "* Parameter Initialization\n",
    "* Activation functions and their gradients\n",
    "* Forward propagation\n",
    "* Backward propagation\n",
    "\n",
    "These functions will be used to complete parts **(b)**\n",
    "and **(c)** of Homework 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seed = 3\n",
    "test_layer_dims = [5,4,3]\n",
    "test_activations = [\"relu\", \"relu\", \"sigmoid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_test_case():\n",
    "    m = (np.array([[ 1.62434536, -0.61175641],\n",
    "        [-0.52817175, -1.07296862],\n",
    "        [ 0.86540763, -2.3015387 ]]),\n",
    "     np.array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]),\n",
    "     np.array([[-0.24937038]])) \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward_test_case():\n",
    "    m = (np.array([[-0.41675785, -0.05626683],\n",
    "        [-2.1361961 ,  1.64027081],\n",
    "        [-1.79343559, -0.84174737]]),\n",
    "     np.array([[ 0.50288142, -1.24528809, -1.05795222]]),\n",
    "     np.array([[-0.90900761]]))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward_test_case_2hidden():\n",
    "    m = (np.array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],\n",
    "        [-2.48678065,  0.91325152,  1.12706373, -1.51409323],\n",
    "        [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],\n",
    "        [-0.33588161,  1.23773784,  0.11112817,  0.12915125],\n",
    "        [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]]),\n",
    "     {'W1': np.array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],\n",
    "         [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],\n",
    "         [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],\n",
    "         [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]),\n",
    "      'W2': np.array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],\n",
    "         [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],\n",
    "         [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]),\n",
    "      'W3': np.array([[ 0.9398248 ,  0.42628539, -0.75815703]]),\n",
    "      'b1': np.array([[ 1.38503523],\n",
    "         [-0.51962709],\n",
    "         [-0.78015214],\n",
    "         [ 0.95560959]]),\n",
    "      'b2': np.array([[ 1.50278553],\n",
    "         [-0.59545972],\n",
    "         [ 0.52834106]]),\n",
    "      'b3': np.array([[-0.16236698]])})\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_test_case():\n",
    "    m = (np.array([[1, 1, 1]]), \n",
    "         np.array([[ 0.8,  0.9,  0.4]]))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward_test_case():\n",
    "    m = (np.array([[ 1.62434536, -0.61175641]]), \n",
    "         (np.array([[-0.52817175, -1.07296862],\n",
    "         [ 0.86540763, -2.3015387 ],\n",
    "         [ 1.74481176, -0.7612069 ]]),\n",
    "      np.array([[ 0.3190391 , -0.24937038,  1.46210794]]),\n",
    "      np.array([[-2.06014071]])))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward_test_case():\n",
    "    m = (np.array([[-0.41675785, -0.05626683]]), \n",
    "         ((np.array([[-2.1361961 ,  1.64027081],\n",
    "          [-1.79343559, -0.84174737],\n",
    "          [ 0.50288142, -1.24528809]]),\n",
    "       np.array([[-1.05795222, -0.90900761,  0.55145404]]),\n",
    "       np.array([[ 2.29220801]])),\n",
    "      np.array([[ 0.04153939, -1.11792545]])))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward_test_case():\n",
    "    m = (np.array([[ 1.78862847,  0.43650985]]),\n",
    "     np.array([[1, 0]]),\n",
    "     (((np.array([[ 0.09649747, -1.8634927 ],\n",
    "           [-0.2773882 , -0.35475898],\n",
    "           [-0.08274148, -0.62700068],\n",
    "           [-0.04381817, -0.47721803]]),\n",
    "        np.array([[-1.31386475,  0.88462238,  0.88131804,  1.70957306],\n",
    "           [ 0.05003364, -0.40467741, -0.54535995, -1.54647732],\n",
    "           [ 0.98236743, -1.10106763, -1.18504653, -0.2056499 ]]),\n",
    "        np.array([[ 1.48614836],\n",
    "           [ 0.23671627],\n",
    "           [-1.02378514]])),\n",
    "       np.array([[-0.7129932 ,  0.62524497],\n",
    "          [-0.16051336, -0.76883635],\n",
    "          [-0.23003072,  0.74505627]])),\n",
    "      ((np.array([[ 1.97611078, -1.24412333],\n",
    "           [-0.62641691, -0.80376609],\n",
    "           [-2.41908317, -0.92379202]]),\n",
    "        np.array([[-1.02387576,  1.12397796, -0.13191423]]),\n",
    "        np.array([[-1.62328545]])),\n",
    "       np.array([[ 0.64667545, -0.35627076]]))))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_test_case():\n",
    "    m = ({'W1': np.array([[-0.41675785, -0.05626683, -2.1361961 ,  1.64027081],\n",
    "         [-1.79343559, -0.84174737,  0.50288142, -1.24528809],\n",
    "         [-1.05795222, -0.90900761,  0.55145404,  2.29220801]]),\n",
    "      'W2': np.array([[-0.5961597 , -0.0191305 ,  1.17500122]]),\n",
    "      'b1': np.array([[ 0.04153939],\n",
    "         [-1.11792545],\n",
    "         [ 0.53905832]]),\n",
    "      'b2': np.array([[-0.74787095]])},\n",
    "     {'dW1': np.array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],\n",
    "         [-0.2773882 , -0.35475898, -0.08274148, -0.62700068],\n",
    "         [-0.04381817, -0.47721803, -1.31386475,  0.88462238]]),\n",
    "      'dW2': np.array([[-0.40467741, -0.54535995, -1.54647732]]),\n",
    "      'db1': np.array([[ 0.88131804],\n",
    "         [ 1.70957306],\n",
    "         [ 0.05003364]]),\n",
    "      'db2': np.array([[ 0.98236743]])})\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Intitializtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, seed=42):\n",
    "    \"\"\" Initialize parameters for each layer in NN\n",
    "    \n",
    "    :param layer_dims: dimensions for each layer\n",
    "    :param seed: int to set random seed\n",
    "    \n",
    "    :return: weight matrices W and bias vectors b\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        parameters['W' + str(l)] = \\\n",
    "        np.random.randn(layer_dims[l], \n",
    "                        layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = \\\n",
    "        np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "    assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(test_layer_dims, seed=test_seed)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "* Sigmoid $F(Z)$, range $[0,1]$\n",
    "\n",
    "    $$ \\begin{align*} F(Z) &= \\frac{1}{1 + e^{-Z}} = \\sigma(Z) \\\\\n",
    "     F^{\\prime}(Z) &= F(X)(1 - F(Z)) \\end{align*}$$\n",
    "    \n",
    "* Tanh $F(Z)$, range $[-1, 1]$\n",
    "\n",
    "    $$\\begin{align*} F(Z) &= \\frac{e^Z - e^{-Z}}{e^Z + e^{-Z}} = \\tanh(Z) \\\\\n",
    "       F^{\\prime}(Z) &= 1 - F(Z)^2 \\end{align*}$$\n",
    "       \n",
    "* Relu $F(Z)$, range $[0, +\\infty]$\n",
    "\n",
    "    $$\\begin{align*} F(Z) &= \\max(0,Z) \\\\\n",
    "       F^{\\prime}(Z) &= \\begin{cases} 1, & \\text{ if } Z > 0 \\\\\n",
    "                         \\text{undefined}, & \\text{ if } Z = 0 \\\\\n",
    "                         0, & \\text{ if } Z < 0 \\end{cases} \n",
    "                         \\end{align*}$$\n",
    "                         \n",
    "Compute gradient for an activation function\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) $$\n",
    "\n",
    "TODO: verify gradient equation from notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\" Sigmoid activation function\n",
    "    \n",
    "    :param Z: -- the input of the activation function\n",
    "    \n",
    "    :return: sigmoid function applied to vector Z\n",
    "    \"\"\"\n",
    "    A = (1 + np.exp(-Z))**(-1)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out how the activation_cache works\n",
    "\n",
    "def sigmoid_gradient(dA, cache):\n",
    "    \"\"\" Gradient of sigmoid function \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    s = np.power((1 + np.exp(-Z)),-1)\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\" Relu activation function\n",
    "    \n",
    "    :param Z: -- the input of the activation function\n",
    "    \n",
    "    :return: relu function applied to vector Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(Z, 0)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_gradient(dA, cache):\n",
    "    \"\"\" Gradient of the relu function \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\" Tanh activation function\n",
    "    \n",
    "    :param Z: -- the input of the activation function\n",
    "    \n",
    "    :return: tanh function applied to vector Z\n",
    "    \"\"\"\n",
    "    e_z = np.exp(Z)\n",
    "    e_nz = np.exp(-Z)\n",
    "    A = (e_z - e_nz)/(e_z + e_nz)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_gradient(Z, cache):\n",
    "    \"\"\" Gradient of the tanh function \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    e_z = np.exp(Z)\n",
    "    e_nz = np.exp(-Z)\n",
    "    s = (e_z - e_nz)/(e_z + e_nz)    \n",
    "    A = Z * (1 - np.power(s, 2))\n",
    "\n",
    "    return A, Z\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "Implementing forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\" Linear part of a layer's forward propagation \n",
    "    \n",
    "    :param A: activation from previous layer (or input data)\n",
    "    :param W: weights matrix \n",
    "    :param b: bias vector\n",
    "    \n",
    "    :return: Z -- the input of the activation function\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295336 -1.23429988]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\" Forward propagation for the LINEAR->ACTIVATION layer\n",
    "    \n",
    "    :param A_prev: activation from previous layer (or input data)\n",
    "    :param W: weights matrix\n",
    "    :param b: bias vector\n",
    "    :param activation: activation function name of \"sigmoid\", \n",
    "                \"relu\", or \"tanh\"\n",
    "                \n",
    "    :return: A -- output of the activation function\n",
    "    :return: cache -- contains \"linear_cache\" and \"activation_cache\"\n",
    "    \"\"\"\n",
    "    Z, linear_cache =  linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        A, activation_cache = tanh(Z)\n",
    "    \n",
    "    else:\n",
    "        raise(TypeError(\"Activation done not exist: {}\".\\\n",
    "                        format(activation)))\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return (A, cache)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896134 0.        ]]\n",
      "With tanh: A = [[ 0.99794156 -0.96982745]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"tanh\")\n",
    "print(\"With tanh: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, activations):\n",
    "    \"\"\" Forward propogation given a model specification \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) //2 # Number of layers in the neural network\n",
    "    \n",
    "    assert L == len(activations)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        activation = activations.popleft()\n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], \n",
    "                                             parameters[\"b\" + str(l)], \n",
    "                                             activation = activation)\n",
    "        caches.append(cache)\n",
    "\n",
    "    activation = activations.popleft()\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(l + 1)], \n",
    "                                          parameters[\"b\" + str(l + 1)], \n",
    "                                          activation = activation)\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return (AL, caches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "activations = deque([\"relu\", \"relu\", \"sigmoid\"])\n",
    "AL, caches = L_model_forward(X, parameters, activations)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Using cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n",
    "TODO: May need a different cost function depending on the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Cross entropy cost J\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    a = np.multiply(Y, np.log(AL))\n",
    "    b = np.multiply((1 - Y), np.log(1 - AL))\n",
    "    cost = - 1 * np.sum(a + b, axis=1) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.414931599615397\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "\n",
    "Implementing backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\" Linear portion of backward propagation for a single layer \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return (dA_prev, dW, db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506362  0.15255393]\n",
      " [ 2.37496825 -0.8944539 ]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992504]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\" Backward propagation for the LINEAR->ACTIVATION layer \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_gradient(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_gradient(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_gradient(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return (dA_prev, dW, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.0110534 ]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576155]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513825  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "#dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"tanh\")\n",
    "#print (\"tanh:\")\n",
    "#print (\"dA_prev = \"+ str(dA_prev))\n",
    "#print (\"dW = \" + str(dW))\n",
    "#print (\"db = \" + str(db))\n",
    "\n",
    "# TODO: fix tanh activation function (minor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, activations):\n",
    "    \"\"\" Implement backward propagation for the specified model \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initialize the backpropagation\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer gradients\n",
    "    \n",
    "    activation = activations.pop()\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \\\n",
    "    linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        activation = activations.pop()\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        dA_prev_temp, dW_temp, db_temp = \\\n",
    "        linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dA1': array([[ 0.12913162, -0.44014127],\n",
      "       [-0.14175655,  0.48317296],\n",
      "       [ 0.01663708, -0.05670697]]), 'dW2': array([[-0.39202432, -0.13325855, -0.04601089]]), 'db2': array([[0.15187861]]), 'dA0': array([[ 0.        ,  0.52257901],\n",
      "       [ 0.        , -0.3269206 ],\n",
      "       [ 0.        , -0.32070404],\n",
      "       [ 0.        , -0.74079187]]), 'dW1': array([[0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.05283652, 0.01005865, 0.01777766, 0.0135308 ]]), 'db1': array([[-0.22007063],\n",
      "       [ 0.        ],\n",
      "       [-0.02835349]])}\n"
     ]
    }
   ],
   "source": [
    "activations = [\"relu\", \"sigmoid\"]\n",
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "\n",
    "grads = L_model_backward(AL, Y_assess, caches, activations)\n",
    "\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters\n",
    "\n",
    "Update parameters of the model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\" Update parameters using gradient descent \"\"\"\n",
    "    L = len(parameters) // 2 # number of parameters in model\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "        learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "        learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.5956207  -0.09991781 -2.14584585  1.82662008]\n",
      " [-1.76569677 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284051  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888276]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, config):\n",
    "    \"\"\" Run an L-Layer neural network using config file \"\"\"\n",
    "    \n",
    "    # Unpack configuration file\n",
    "    \n",
    "    activations = config.get(\"activations\")\n",
    "    layers_dims = config.get(\"layers_dims\") \n",
    "    learning_rate = config.get(\"learning_rate\")\n",
    "    num_iterations = config.get(\"num_iterations\")\n",
    "    print_cost = config.get(\"print_cost\")\n",
    "    random_seed = config.get(\"random_seed\")\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    costs = [] # keep track of costs\n",
    "    \n",
    "    # Initialize model\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        # Forward propagation\n",
    "        \n",
    "        fact = deque(activations.copy())\n",
    "        AL, caches = L_model_forward(X, parameters, fact)\n",
    "        \n",
    "        # Compute cost\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        \n",
    "        bact = deque(activations.copy())\n",
    "        grads = L_model_backward(AL, Y, caches, bact)\n",
    "        \n",
    "        # Update parameters\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Track cost and print values if specified\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            #print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            pass\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append((i, cost))\n",
    "            \n",
    "    if print_cost:\n",
    "        costs.append((i, cost))\n",
    "             \n",
    "    return parameters, costs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activations):\n",
    "    \"\"\" Prediction using params from neural network \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    \n",
    "    facts = deque(activations.copy())\n",
    "    probas, caches = L_model_forward(X, parameters, facts)\n",
    "    \n",
    "    # Convert probas to 0/1 predictions\n",
    "    \n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "            \n",
    "    #print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_model(meta_config:dict):\n",
    "    \"\"\" Generate a model report based on meta_config file \"\"\"\n",
    "    \n",
    "    config_template = meta_config.get(\"config_template\", None)\n",
    "    learning_range = meta_config.get(\"learning_range\", None)\n",
    "    learning_step = meta_config.get(\"learning_step\", None)\n",
    "    X_train = meta_config.get(\"X_train\", None)\n",
    "    y_train = meta_config.get(\"y_train\", None)\n",
    "    X_test = meta_config.get(\"X_test\", None)\n",
    "    y_test = meta_config.get(\"y_test\", None)\n",
    "    X_validate = meta_config.get(\"X_validate\", None)\n",
    "    y_validate = meta_config.get(\"y_validate\", None) \n",
    "    \n",
    "    learning_min, learning_max = learning_range\n",
    "    \n",
    "    learning_rates = np.arange(learning_min, learning_max, learning_step)\n",
    "    reports = []\n",
    "    report = {}\n",
    "    \n",
    "    i = 0\n",
    "    for learning_rate in learning_rates:\n",
    "        \n",
    "        config = config_template.copy()\n",
    "        config[\"learning_rate\"] = learning_rate\n",
    "        \n",
    "        # Train the model\n",
    "        \n",
    "        parameters, costs = train_model(X_train, y_train, config)\n",
    "        \n",
    "        # Predict on training set\n",
    "        train_score = predict(X_train, y_train, parameters, \n",
    "                              config.get(\"activations\"))\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_score = predict(X_test, y_test, parameters, \n",
    "                             config.get(\"activations\"))\n",
    "        \n",
    "        # Predict on validation set\n",
    "        validation_score = predict(X_validate, y_validate, \n",
    "                                   parameters, config.get(\"activations\"))\n",
    "        \n",
    "        model_report = report.copy()\n",
    "        model_report[\"learning_rate\"] = learning_rate\n",
    "        model_report[\"costs\"] = costs\n",
    "        model_report[\"train_parameters\"] = parameters\n",
    "        model_report[\"train_score\"] = train_score\n",
    "        model_report[\"test_score\"] = test_score\n",
    "        model_report[\"validation_score\"] = validation_score\n",
    "        \n",
    "        reports.append(model_report)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Computing the {}th model\".format(i))\n",
    "            \n",
    "        i += 1\n",
    "            \n",
    "    return reports\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to figure out config for the data you have available\n",
    "\n",
    "config = {\n",
    "    \"activations\": [\"relu\", \"sigmoid\"],\n",
    "    \"layers_dims\" : [2, 2, 1], #  2-layer model\n",
    "    \"learning_rate\": 0.0075,\n",
    "    \"num_iterations\" : 3000,\n",
    "    \"print_cost\": True,\n",
    "    \"random_seed\": random_state,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = train_model(X_train.T.values, y_train.T.values, config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = predict(X_test.T.values, y_test.T.values, parameters, config.get(\"activations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_config = {\n",
    "    \"config_template\" : config,\n",
    "    \"learning_range\": (0.0005, 0.01),\n",
    "    \"learning_step\": 0.0005,\n",
    "    \"X_train\": X_train.T.values,\n",
    "    \"y_train\": y_train.T.values,\n",
    "    \"X_test\": X_test.T.values,\n",
    "    \"y_test\": y_test.T.values,\n",
    "    \"X_validate\": X_val.T.values,\n",
    "    \"y_validate\": y_val.T.values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the 0th model\n"
     ]
    }
   ],
   "source": [
    "reports_partb = report_model(meta_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reports_partb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
