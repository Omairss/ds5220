{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network\n",
    "\n",
    "Please note that I've already completed the \"Neural Networks and Deep Learning\" course from Andrew Ng. This is the course which was recommended in class notes. Much of my code will look similar to what I did for that assignment. However, the activation functions and their use in backpropagation will be unique. I'm kind of overbuilding in some ways but this serves the purpose of being able to use my implementation here to complete a stage of our research project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network Implementation\n",
    "\n",
    "The following functions implement a Feedforward Neural Network. These\n",
    "functions are broken out into a few sections\n",
    "\n",
    "* Parameter Initialization\n",
    "* Activation functions and their gradients\n",
    "* Forward propagation\n",
    "* Backward propagation\n",
    "\n",
    "These functions will be used to complete parts **(b)**\n",
    "and **(c)** of Homework 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Intitializtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, seed=42):\n",
    "    \"\"\" Initialize parameters for each layer in NN\n",
    "    \n",
    "    :param layer_dims: dimensions for each layer\n",
    "    :param seed: int to set random seed\n",
    "    \n",
    "    :return: weight matrices W and bias vectors b\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        parameters['W' + str(l)] = \\\n",
    "        np.random.randn(layer_dims[l], \n",
    "                        layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = \\\n",
    "        np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "    assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "* Sigmoid $F(Z)$, range $[0,1]$\n",
    "\n",
    "    $$ \\begin{align*} F(Z) &= \\frac{1}{1 + e^{-Z}} = \\sigma(Z) \\\\\n",
    "     F^{\\prime}(Z) &= F(X)(1 - F(Z)) \\end{align*}$$\n",
    "    \n",
    "* Tanh $F(Z)$, range $[-1, 1]$\n",
    "\n",
    "    $$\\begin{align*} F(Z) &= \\frac{e^Z - e^{-Z}}{e^Z + e^{-Z}} = \\tanh(Z) \\\\\n",
    "       F^{\\prime}(Z) &= 1 - F(Z)^2 \\end{align*}$$\n",
    "       \n",
    "* Relu $F(Z)$, range $[0, +\\infty]$\n",
    "\n",
    "    $$\\begin{align*} F(Z) &= \\max(0,Z) \\\\\n",
    "       F^{\\prime}(Z) &= \\begin{cases} 1, & \\text{ if } Z > 0 \\\\\n",
    "                         \\text{undefined}, & \\text{ if } Z = 0 \\\\\n",
    "                         0, & \\text{ if } Z < 0 \\end{cases} \n",
    "                         \\end{align*}$$\n",
    "                         \n",
    "Compute gradient for an activation function\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) $$\n",
    "\n",
    "TODO: verify gradient equation from notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\" Sigmoid activation function\n",
    "    \n",
    "    :param Z: -- the input of the activation function\n",
    "    \n",
    "    :return: sigmoid function applied to vector Z\n",
    "    \"\"\"\n",
    "    A = (1 + np.exp(-Z))**(-1)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: figure out how the activation_cache works\n",
    "\n",
    "def sigmoid_gradient(dA, cache):\n",
    "    \"\"\" Gradient of sigmoid function \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    s = np.power((1 + np.exp(-Z)),-1)\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\" Relu activation function\n",
    "    \n",
    "    :param Z: -- the input of the activation function\n",
    "    \n",
    "    :return: relu function applied to vector Z\n",
    "    \"\"\"\n",
    "    A = np.maximum(Z, 0)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_gradient(dA, cache):\n",
    "    \"\"\" Gradient of the relu function \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\" Tanh activation function\n",
    "    \n",
    "    :param Z: -- the input of the activation function\n",
    "    \n",
    "    :return: tanh function applied to vector Z\n",
    "    \"\"\"\n",
    "    e_z = np.exp(Z)\n",
    "    e_nz = np.exp(-Z)\n",
    "    A = (e_z - e_nz)/(e_z + e_nz)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_gradient(Z, cache):\n",
    "    \"\"\" Gradient of the tanh function \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    e_z = np.exp(Z)\n",
    "    e_nz = np.exp(-Z)\n",
    "    s = (e_z - e_nz)/(e_z + e_nz)    \n",
    "    A = Z * (1 - np.power(s, 2))\n",
    "\n",
    "    return A, Z\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "Implementing forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\" Linear part of a layer's forward propagation \n",
    "    \n",
    "    :param A: activation from previous layer (or input data)\n",
    "    :param W: weights matrix \n",
    "    :param b: bias vector\n",
    "    \n",
    "    :return: Z -- the input of the activation function\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\" Forward propagation for the LINEAR->ACTIVATION layer\n",
    "    \n",
    "    :param A_prev: activation from previous layer (or input data)\n",
    "    :param W: weights matrix\n",
    "    :param b: bias vector\n",
    "    :param activation: activation function name of \"sigmoid\", \n",
    "                \"relu\", or \"tanh\"\n",
    "                \n",
    "    :return: A -- output of the activation function\n",
    "    :return: cache -- contains \"linear_cache\" and \"activation_cache\"\n",
    "    \"\"\"\n",
    "    Z, linear_cache =  linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        A, activation_cache = tanh(Z)\n",
    "    \n",
    "    else:\n",
    "        raise(TypeError(\"Activation done not exist: {}\".\\\n",
    "                        format(activation)))\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return (A, cache)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, activations):\n",
    "    \"\"\" Forward propogation given a model specification \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) //2 # Number of layers in the neural network\n",
    "    \n",
    "    assert L == len(activations)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        activation = activations.popleft()\n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], \n",
    "                                             parameters[\"b\" + str(l)], \n",
    "                                             activation = activation)\n",
    "        caches.append(cache)\n",
    "\n",
    "    activation = activations.popleft()\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(l + 1)], \n",
    "                                          parameters[\"b\" + str(l + 1)], \n",
    "                                          activation = activation)\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return (AL, caches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Using cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n",
    "TODO: May need a different cost function depending on the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Cross entropy cost J\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    a = np.multiply(Y, np.log(AL))\n",
    "    b = np.multiply((1 - Y), np.log(1 - AL))\n",
    "    cost = - 1 * np.sum(a + b, axis=1) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "\n",
    "Implementing backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\" Linear portion of backward propagation for a single layer \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return (dA_prev, dW, db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\" Backward propagation for the LINEAR->ACTIVATION layer \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_gradient(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_gradient(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_gradient(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return (dA_prev, dW, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, activations):\n",
    "    \"\"\" Implement backward propagation for the specified model \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initialize the backpropagation\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer gradients\n",
    "    \n",
    "    activation = activations.pop()\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \\\n",
    "    linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        activation = activations.pop()\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        dA_prev_temp, dW_temp, db_temp = \\\n",
    "        linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters\n",
    "\n",
    "Update parameters of the model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\" Update parameters using gradient descent \"\"\"\n",
    "    L = len(parameters) // 2 # number of parameters in model\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "        learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "        learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(X, Y, config):\n",
    "    \"\"\" Run an L-Layer neural network using config file \"\"\"\n",
    "    \n",
    "    # Unpack configuration file\n",
    "    \n",
    "    activations = config.get(\"activations\")\n",
    "    layers_dims = config.get(\"layers_dims\") \n",
    "    learning_rate = config.get(\"learning_rate\")\n",
    "    num_iterations = config.get(\"num_iterations\")\n",
    "    print_cost = config.get(\"print_cost\")\n",
    "    random_seed = config.get(\"random_seed\")\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    costs = [] # keep track of costs\n",
    "    \n",
    "    # Initialize model\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        # Forward propagation\n",
    "        \n",
    "        fact = deque(activations.copy())\n",
    "        AL, caches = L_model_forward(X, parameters, fact)\n",
    "        \n",
    "        # Compute cost\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        \n",
    "        bact = deque(activations.copy())\n",
    "        grads = L_model_backward(AL, Y, caches, bact)\n",
    "        \n",
    "        # Update parameters\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Track cost and print values if specified\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append((i, cost))\n",
    "            \n",
    "    if print_cost:\n",
    "        costs.append((i, cost))\n",
    "             \n",
    "    return parameters, costs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activations):\n",
    "    \"\"\" Prediction using params from neural network \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    \n",
    "    facts = deque(activations.copy())\n",
    "    probas, caches = L_model_forward(X, parameters, facts)\n",
    "    \n",
    "    # Convert probas to 0/1 predictions\n",
    "    \n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "            \n",
    "    #print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_model(meta_config:dict):\n",
    "    \"\"\" Generate a model report based on meta_config file \"\"\"\n",
    "    \n",
    "    config_template = meta_config.get(\"config_template\", None)\n",
    "    learning_range = meta_config.get(\"learning_range\", None)\n",
    "    learning_step = meta_config.get(\"learning_step\", None)\n",
    "    X_train = meta_config.get(\"X_train\", None)\n",
    "    y_train = meta_config.get(\"y_train\", None)\n",
    "    X_test = meta_config.get(\"X_test\", None)\n",
    "    y_test = meta_config.get(\"y_test\", None)\n",
    "    X_validate = meta_config.get(\"X_validate\", None)\n",
    "    y_validate = meta_config.get(\"y_validate\", None) \n",
    "    \n",
    "    learning_min, learning_max = learning_range\n",
    "    \n",
    "    learning_rates = np.arange(learning_min, learning_max, learning_step)\n",
    "    reports = []\n",
    "    report = {}\n",
    "    \n",
    "    i = 0\n",
    "    for learning_rate in learning_rates:\n",
    "        \n",
    "        config = config_template.copy()\n",
    "        config[\"learning_rate\"] = learning_rate\n",
    "        \n",
    "        # Train the model\n",
    "        \n",
    "        parameters, costs = train_model(X_train, y_train, config)\n",
    "        \n",
    "        # Predict on training set\n",
    "        train_score = predict(X_train, y_train, parameters, \n",
    "                              config.get(\"activations\"))\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_score = predict(X_test, y_test, parameters, \n",
    "                             config.get(\"activations\"))\n",
    "        \n",
    "        # Predict on validation set\n",
    "        validation_score = predict(X_validate, y_validate, \n",
    "                                   parameters, config.get(\"activations\"))\n",
    "        \n",
    "        model_report = report.copy()\n",
    "        model_report[\"learning_rate\"] = learning_rate\n",
    "        model_report[\"costs\"] = costs\n",
    "        model_report[\"train_parameters\"] = parameters\n",
    "        model_report[\"train_score\"] = train_score\n",
    "        model_report[\"test_score\"] = test_score\n",
    "        model_report[\"validation_score\"] = validation_score\n",
    "        \n",
    "        reports.append(model_report)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Computing the {}th model\".format(i))\n",
    "            \n",
    "        i += 1\n",
    "            \n",
    "    return reports\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filepath, y_label_value):\n",
    "    x = np.load(filepath).transpose()\n",
    "    y = np.full(x.shape[0], y_label_value)\n",
    "    y = y.reshape(y.shape[0],1)\n",
    "    print(\"X shape {}\".format(x.shape))\n",
    "    input_data = np.append(x, y, axis=1) \n",
    "    print(\"Input shape {}\".format(input_data.shape))\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_val_test(x, y):\n",
    "    x_train, x_intermediate, y_train, y_intermediate = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    print(\"Train shape x: {}, y:{}\".format(x_train.shape, y_intermediate.shape))\n",
    "    print(\"Intermediate shape x: {}, y:{}\".format(x_intermediate.shape, y_train.shape))\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_intermediate, y_intermediate, test_size=0.5, random_state=42)\n",
    "    print(\"Val shape x: {}, y:{}\".format(x_val.shape, y_val.shape))\n",
    "    print(\"Test shape x: {}, y:{}\".format(x_test.shape, y_test.shape))\n",
    "    return(x_train, y_train, x_val, y_val, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    filepath_fish = \"/Users/karangm/Downloads/fish_1.npy\"\n",
    "    input_fish = load_data(filepath_fish, 0)\n",
    "    filepath_fork = \"/Users/karangm/Downloads/fork_1.npy\"\n",
    "    input_fork = load_data(filepath_fork, 1)\n",
    "    input_data = np.concatenate((input_fish, input_fork), axis=0)\n",
    "    np.random.shuffle(input_data)\n",
    "    print(\"Input shape {}\".format(input_data.shape))\n",
    "    \n",
    "    ## Split into x and y\n",
    "    x = input_data[:, :65536]\n",
    "    y = input_data[:, 65536]\n",
    "    \n",
    "    ## Split into train(80), validation(10), test(10)\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = create_train_val_test(x, y)\n",
    "    \n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_val = y_val.reshape(y_val.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    \n",
    "    return(x_train, y_train, x_val, y_val, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (1000, 65536)\n",
      "Input shape (1000, 65537)\n",
      "X shape (1000, 65536)\n",
      "Input shape (1000, 65537)\n",
      "Input shape (2000, 65537)\n",
      "Train shape x: (1600, 65536), y:(400,)\n",
      "Intermediate shape x: (400, 65536), y:(1600,)\n",
      "Val shape x: (200, 65536), y:(200,)\n",
      "Test shape x: (200, 65536), y:(200,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to figure out config for the data you have available\n",
    "random_state = 42\n",
    "config = {\n",
    "    \"activations\": [\"relu\", \"sigmoid\"],\n",
    "    \"layers_dims\" : [65536, 50, 1], #  2-layer model\n",
    "    \"learning_rate\": 0.0075,\n",
    "    \"num_iterations\" : 3000,\n",
    "    \"print_cost\": True,\n",
    "    \"random_seed\": random_state,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693155\n",
      "Cost after iteration 50: 0.693078\n",
      "Cost after iteration 100: 0.693011\n",
      "Cost after iteration 150: 0.692952\n",
      "Cost after iteration 200: 0.692899\n",
      "Cost after iteration 250: 0.692852\n",
      "Cost after iteration 300: 0.692809\n",
      "Cost after iteration 350: 0.692770\n",
      "Cost after iteration 400: 0.692733\n",
      "Cost after iteration 450: 0.692699\n",
      "Cost after iteration 500: 0.692666\n",
      "Cost after iteration 550: 0.692635\n",
      "Cost after iteration 600: 0.692605\n",
      "Cost after iteration 650: 0.692576\n",
      "Cost after iteration 700: 0.692548\n",
      "Cost after iteration 750: 0.692519\n",
      "Cost after iteration 800: 0.692491\n",
      "Cost after iteration 850: 0.692463\n",
      "Cost after iteration 900: 0.692435\n",
      "Cost after iteration 950: 0.692407\n",
      "Cost after iteration 1000: 0.692378\n",
      "Cost after iteration 1050: 0.692349\n",
      "Cost after iteration 1100: 0.692319\n",
      "Cost after iteration 1150: 0.692289\n",
      "Cost after iteration 1200: 0.692258\n",
      "Cost after iteration 1250: 0.692227\n",
      "Cost after iteration 1300: 0.692194\n",
      "Cost after iteration 1350: 0.692161\n",
      "Cost after iteration 1400: 0.692127\n",
      "Cost after iteration 1450: 0.692092\n",
      "Cost after iteration 1500: 0.692055\n",
      "Cost after iteration 1550: 0.692018\n",
      "Cost after iteration 1600: 0.691979\n",
      "Cost after iteration 1650: 0.691939\n",
      "Cost after iteration 1700: 0.691898\n",
      "Cost after iteration 1750: 0.691855\n",
      "Cost after iteration 1800: 0.691811\n",
      "Cost after iteration 1850: 0.691765\n",
      "Cost after iteration 1900: 0.691718\n",
      "Cost after iteration 1950: 0.691668\n",
      "Cost after iteration 2000: 0.691617\n",
      "Cost after iteration 2050: 0.691565\n",
      "Cost after iteration 2100: 0.691510\n",
      "Cost after iteration 2150: 0.691453\n",
      "Cost after iteration 2200: 0.691394\n",
      "Cost after iteration 2250: 0.691332\n",
      "Cost after iteration 2300: 0.691269\n",
      "Cost after iteration 2350: 0.691203\n",
      "Cost after iteration 2400: 0.691134\n",
      "Cost after iteration 2450: 0.691062\n",
      "Cost after iteration 2500: 0.690988\n",
      "Cost after iteration 2550: 0.690911\n",
      "Cost after iteration 2600: 0.690831\n",
      "Cost after iteration 2650: 0.690747\n",
      "Cost after iteration 2700: 0.690660\n",
      "Cost after iteration 2750: 0.690570\n",
      "Cost after iteration 2800: 0.690476\n",
      "Cost after iteration 2850: 0.690378\n",
      "Cost after iteration 2900: 0.690276\n",
      "Cost after iteration 2950: 0.690170\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = train_model(x_train.T, y_train.T, config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = predict(x_test.T, y_test.T, parameters, config.get(\"activations\"))\n",
    "y_test_pred = y_test_pred.T \n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44500000000000001"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_config = {\n",
    "    \"config_template\" : config,\n",
    "    \"learning_range\": (0.0005, 0.01),\n",
    "    \"learning_step\": 0.0005,\n",
    "    \"X_train\": x_train.T,\n",
    "    \"y_train\": y_train.T,\n",
    "    \"X_test\": x_test.T,\n",
    "    \"y_test\": y_test.T,\n",
    "    \"X_validate\": x_val.T,\n",
    "    \"y_validate\": y_val.T,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693155\n",
      "Cost after iteration 50: 0.693150\n",
      "Cost after iteration 100: 0.693144\n",
      "Cost after iteration 150: 0.693139\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8adedf69a195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreports_partb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-b916abe97f4e>\u001b[0m in \u001b[0;36mreport_model\u001b[0;34m(meta_config)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Predict on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-88e4bd8b639f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, Y, config)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-5a98e0b258a4>\u001b[0m in \u001b[0;36mL_model_backward\u001b[0;34m(AL, Y, caches, activations)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdA_prev_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_temp\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mlinear_activation_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA_prev_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-29370e71b5b1>\u001b[0m in \u001b[0;36mlinear_activation_backward\u001b[0;34m(dA, cache, activation)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ddbdc2ca5c2f>\u001b[0m in \u001b[0;36mlinear_backward\u001b[0;34m(dZ, cache)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reports_partb = report_model(meta_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reports_partb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
